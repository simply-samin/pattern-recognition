# -*- coding: utf-8 -*-
"""KNN And Text Processing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11mXljqL9VtDK4mL88rsR4sHfqw54qM-0
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
dir = "/content/drive/MyDrive/Colab Notebooks/Datasets/offline_3"
# %cd {dir}

import math
import random

import numpy as np
import pandas as pd

import string
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
# from bs4 import BeautifulSoup as bs
# import lxml

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from sklearn.feature_extraction.text import CountVectorizer

import enum

class Metric(enum.Enum):
    EUCLIDEAN_DISTANCE = "euclidean"
    HAMMING_DISTANCE = "hamming"
    MANHATTAN_DISTANCE = "manhattan"
    COSINE_SIMILARITY = "cosine"

class kNN_Texts:

    ### DO NOT change anything in the constructor
    def __init__(self, vectorizer_class=None, 
                 K=None,
                 metric=Metric.EUCLIDEAN_DISTANCE):
        
        self.__vectorizer = vectorizer_class(analyzer=lambda text: text)
        # self.__vectorizer = vectorizer_class(analyzer=self.__preprocess_text)
        
        self.__K = K
        self.__metric = metric

        self.__train_vocabulary = None
        self.__train_feature_vectors = None
        self.__train_labels = None

    """@staticmethod"""
    def __preprocess_text(self, text):
        text = text.lower()
        text = re.sub(r'[-+]?\d+', '', text)
        text = re.sub(r'https?:\/\/\S*', '', text)
        text = re.sub(r'www\.\S*', '', text)
        text = re.sub(r'\S*\.(com|info|net|org)', '', text)
        text = text.translate((str.maketrans('', '', string.punctuation)))
        text = word_tokenize(text)
        stop_words = set(stopwords.words('english'))
        text = [word for word in text if not word in stop_words]
        lemmatizer = WordNetLemmatizer()
        text = [lemmatizer.lemmatize(word) for word in text]
        stemmer = PorterStemmer()
        text = [stemmer.stem(word) for word in text]
        preprocessed_text = text

        return preprocessed_text

    def __fit_vectorizer(self, texts):
        
        self.__vectorizer.fit(texts)
        self.__train_vocabulary = self.__vectorizer.vocabulary_
        print(self.__train_vocabulary)
  
    def __vectorize_texts(self, texts):
        ### TODO
        
        texts_feature_vectors = self.__vectorizer.transform(texts).toarray()
        print("encoded Documents: ")
        print(texts_feature_vectors)

        return texts_feature_vectors

    def __train(self, texts, labels):
        ### TODO
        preprocessed_texts = []
        for text in texts:
          preprocessed_texts.append(self.__preprocess_text(text)) 
        print(preprocessed_texts)
        self.__fit_vectorizer(preprocessed_texts)

          ### TODO
        train_feature_vectors = self.__vectorize_texts(preprocessed_texts)
        
          ### TODO
        train_labels = labels
        
        return train_feature_vectors, train_labels

    def fit(self, texts, labels):
        self.__train_feature_vectors, self.__train_labels = self.__train(texts=texts, 
                                                                         labels=labels)

    def __compute_metric_to_train_points(self, feature_vector):
      from scipy.spatial import distance
      metric_values = None

      dist=[]
      index = None
      ### TODO
      if self.__metric == Metric.EUCLIDEAN_DISTANCE:
        for index, train_feature in enumerate(self.__train_feature_vectors):
          _distance = distance.euclidean(feature_vector, train_feature)
          dist.append([_distance, index])
        metric_values = dist 
      elif self.__metric == Metric.HAMMING_DISTANCE:
        for index, train_feature in enumerate(self.__train_feature_vectors):
          _distance = distance.hamming(feature_vector, train_feature)
          dist.append([_distance, index])
        metric_values = dist 
      elif self.__metric == Metric.MANHATTAN_DISTANCE:
        for index, train_feature in enumerate(self.__train_feature_vectors):
            _distance = distance.cityblock(feature_vector, train_feature)
            dist.append([_distance, index])
        metric_values = dist 
      elif self.__metric == Metric.COSINE_SIMILARITY:
        for index, train_feature in enumerate(self.__train_feature_vectors):
            _distance = distance.cosine(feature_vector, train_feature)
            dist.append([_distance, index])
        metric_values = dist

      return metric_values

    def predict(self, texts):
      ### TODO
      preprocessed_texts = []
      for text in texts:
        preprocessed_texts.append(self.__preprocess_text(text)) 

      ### TODO

      test_feature_vectors = self.__vectorize_texts(preprocessed_texts)
      
      dist = []
      #c = 20
      test_metric_values = []
      predictions = []
      for test_feature_vector in test_feature_vectors:            
        #if(c >= 0):
          #break
        #c = c - 1 
        metric_values = []
        count_1 = 0
        count_0 = 0
        dist = self.__compute_metric_to_train_points(test_feature_vector)

        if self.__metric == Metric.COSINE_SIMILARITY:
          dist.sort(reverse=True)
        else:
          dist.sort()
        
        for i in range(self.__K):          
          if(self.__train_labels[dist[i][1]] == 1):
            count_1 = count_1 + 1
          else:
            count_0 = count_0 + 1
          metric_values.append(dist[i][0])
        
        if(count_1 > count_0):
          predictions.append(1)
        else:
          predictions.append(0)
        
        test_metric_values.append(metric_values)
        
        

      return predictions

clf = kNN_Texts(vectorizer_class=CountVectorizer,
                # metric=Metric.HAMMING_DISTANCE,
                K=3)

df = pd.read_csv("train.csv",
                 na_values="?")
df = df.drop("id", axis=1)
df.dropna(inplace = True)
#df.loc[0:5, :]

data_x = df.loc[:, df.columns != "label"]
data_y = df["label"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_x, data_y,
                                                    train_size=0.8,
                                                    stratify=data_y,
                 random_state=911)
X_train = X_train.values.tolist()
X_test = X_test.values.tolist()
y_train = list(y_train)
y_test = list(y_test)

def flatten_list(_2d_list):
    flat_list = []
    for element in _2d_list:
      flat_list.append(element[0])
    return flat_list

X_train = flatten_list(X_train)
X_test = flatten_list(X_test)

print(X_train)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

y_true = y_test

from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred, labels=[0, 1])

from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred, labels=[0, 1]))

df = pd.read_csv("test.csv",
                 na_values="?")
df = df.drop("id", axis=1)
df.dropna(inplace = True)
test_texts = df.values.tolist()
test_texts = flatten_list(test_texts)

predictions = clf.predict(test_texts)