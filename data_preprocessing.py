# -*- coding: utf-8 -*-
"""PR Lab Data Preprocessing - Class 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19bFKN8fndD_LspDVXFtqw7GammXrx_Wb
"""

from google.colab import drive
drive.mount('/content/drive')

dir = "/content/drive/MyDrive/Colab Notebooks/Datasets/"

# Commented out IPython magic to ensure Python compatibility.
# %cd {dir}

!ls

import pandas as pd
import numpy as np

df = pd.read_csv("adult_modified.csv",
                 na_values="?")

df.loc[0:5, :]

df.isna().sum()

df.loc[df.isna().any(axis=1), :]

df.loc[df["workclass"].isna(), :]

df["income"].unique()

df["income"] = df["income"].str.strip()

df["income"].dtype

from pandas.api.types import is_string_dtype

for col in df.columns:
  if is_string_dtype(df[col].dtype):
    df[col] = df[col].str.strip()

df

from sklearn.model_selection import train_test_split


train_df, test_df = train_test_split(df, 
                                      random_state=911,
                                        train_size=0.9,
                                        stratify=df["income"])

train_x = train_df.loc[:, train_df.columns.drop("income")]
train_y = train_df["income"].copy()

test_df.dropna(inplace=True) ### Drop all TEST rows that have NAs in ANY feature/label. Imputation of the TEST features will not be required then.
# test_df = test_df.loc[np.logical_not(test_df["income"].isna()), :] ### Drop all TEST rows that have NAs in LABEL column. Imputation of the TEST features is necessary then.

test_x = test_df.loc[:, test_df.columns.drop("income")]
test_y = test_df["income"].copy()

test_x.isna().sum()

from sklearn.impute import SimpleImputer

age_imputer = SimpleImputer(strategy='mean')

age_imputer.fit(train_x[["age"]])
train_x["age"] = age_imputer.transform(train_x[["age"]])
# test_x["age"] = age_imputer.transform(test_x[["age"]]).flatten() # Necessary if you kept test rows containing NAs

train_x.isna().sum()

occupation_imputer = SimpleImputer(strategy='most_frequent')
occupation_imputer.fit(train_x[["occupation"]])
train_x["occupation"] = occupation_imputer.transform(train_x[["occupation"]]).flatten()

train_x.isna().sum()

hpw_imputer = SimpleImputer(strategy='mean')
hpw_imputer.fit(train_x[["hours-per-week"]])
train_x["hours-per-week"] = hpw_imputer.transform(train_x[["hours-per-week"]]).flatten()

train_x.isna().sum()

from sklearn.preprocessing import LabelEncoder

income_le = LabelEncoder()
income_le.fit(train_y)
train_y = income_le.transform(train_y)
test_y = income_le.transform(test_y)

income_le.classes_, train_y, test_y

from sklearn.preprocessing import Binarizer

age_bin = Binarizer(threshold=train_x["age"].mean())
age_bin.fit(train_x[["age"]])
train_x["age"] = age_bin.transform(train_x[["age"]]).flatten()
test_x["age"] = age_bin.transform(test_x[["age"]]).flatten()

age_bin.threshold, train_x

from sklearn.preprocessing import KBinsDiscretizer

hpw_kbins = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')
hpw_kbins.fit(train_x[["hours-per-week"]])
train_x["hours-per-week"] = hpw_kbins.transform(train_x[["hours-per-week"]]).flatten()
test_x["hours-per-week"] = hpw_kbins.transform(test_x[["hours-per-week"]]).flatten()

hpw_kbins.bin_edges_, train_x

from sklearn.preprocessing import StandardScaler

fnlwgt_scaler = StandardScaler()

fnlwgt_scaler.fit(train_x[["fnlwgt"]])

train_x["fnlwgt"] = fnlwgt_scaler.transform(train_x[["fnlwgt"]]).flatten()
test_x["fnlwgt"] = fnlwgt_scaler.transform(test_x[["fnlwgt"]]).flatten()

fnlwgt_scaler.mean_, fnlwgt_scaler.var_, train_x

import string
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
# from bs4 import BeautifulSoup as bs
# import lxml

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
 
    #Lowercase the text
    text = text.lower()
 
    #Number Removal
    text = re.sub(r'[-+]?\d+', '', text)
 
    #Remove hyperlinks
    text = re.sub(r'https?:\/\/\S*', '', text)
    #text = re.sub(r'https?:\/\/.*[\r\n]*', '', text)
    #text = re.sub(r'https?:\/\/.*\s*', '', text)
    text = re.sub(r'www\.\S*', '', text)
    text = re.sub(r'\S*\.(com|info|net|org)', '', text)
 
    #Remove punctuations
    text = text.translate((str.maketrans('', '', string.punctuation)))
 
    #Tokenize
    text = word_tokenize(text)
 
    #Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = [word for word in text if not word in stop_words]
 
    #Lemmatize tokens
    lemmatizer = WordNetLemmatizer()
    text = [lemmatizer.lemmatize(word) for word in text]
 
    #Stemming tokens
    stemmer = PorterStemmer()
    text = [stemmer.stem(word) for word in text]
 
    return text

preprocess_text("Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3] It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4] [Taken from https://en.wikipedia.org/wiki/Scikit-learn]")

string.punctuation